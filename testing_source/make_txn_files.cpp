#include <iostream>#include <fstream>#include <string>#include <stdlib.h>#include <math.h>       /* cbrt */#include <vector>#include <algorithm>    // std::minusing namespace std;bool local = false;struct debugLog {  private:    bool on;  public:  debugLog(bool turn_on) {    on = turn_on;  }  void turn_on_logging() {    on = true;  }  void turn_off_logging() {    on = false;  }  template<typename T> debugLog& operator << (const T& x) {   if(on) {      cout << x;    }    return *this;  }  debugLog& operator<<(std::ostream& (*manipulator)(std::ostream&)) {   if(on) {      cout << manipulator;    }    return *this;  }};bool debug_logging = false;bool extreme_debug_logging = false;bool write_hdf5 = true;static debugLog debug_log = debugLog(debug_logging);static debugLog extreme_debug_log = debugLog(extreme_debug_logging);void test_fixed_procs_per_node(uint32_t num_server_procs_per_node_cluster_d_cluster_c_cluster_e, uint32_t num_server_procs_per_node_cluster_a_cluster_b,							   uint32_t num_write_client_procs_per_node_cluster_d_cluster_c_cluster_e, uint32_t num_write_client_procs_per_node_cluster_a_cluster_b,							   uint32_t num_read_client_procs_per_node_cluster_d_cluster_c_cluster_e, uint32_t num_read_client_procs_per_node_cluster_a_cluster_b							   );void generate_txn_file(bool repeat_jobs, string cluster, uint32_t job_num, uint32_t iteration, uint32_t time, uint32_t num_timesteps, 						uint32_t total_nodes, uint32_t num_server_nodes, uint32_t num_write_client_nodes, uint32_t num_read_client_nodes,						uint32_t num_server_procs, uint32_t num_write_client_procs, uint32_t num_read_client_procs, 						uint32_t num_server_procs_per_node, uint32_t num_write_client_procs_per_node, uint32_t num_read_client_procs_per_node,						uint32_t num_write_x_procs, uint32_t num_write_y_procs, uint32_t num_write_z_procs,						uint32_t num_read_x_procs, uint32_t num_read_y_procs, uint32_t num_read_z_procs,						uint64_t nx, uint64_t ny, uint64_t nz						);template <class T>void find_nearest_divisible (T nx, T ny, T nz, T &num_read_procs, T &num_read_x_procs, T &num_read_y_procs, T &num_read_z_procs);template <class T>void find_roots(T value, T &npx, T &npy, T &npz);int main(int argc, char **argv) {	uint32_t num_server_procs_per_node_cluster_d_cluster_c_cluster_e = FILL_IN_DESIRED_VALUE;	uint32_t num_server_procs_per_node_cluster_a_cluster_b = FILL_IN_DESIRED_VALUE;	uint32_t num_write_client_procs_per_node_cluster_d_cluster_c_cluster_e = FILL_IN_DESIRED_VALUE;	uint32_t num_read_client_procs_per_node_cluster_d_cluster_c_cluster_e = FILL_IN_DESIRED_VALUE;	uint32_t num_write_client_procs_per_node_cluster_a_cluster_b = FILL_IN_DESIRED_VALUE;	uint32_t num_read_client_procs_per_node_cluster_a_cluster_b = FILL_IN_DESIRED_VALUE;	test_fixed_procs_per_node(num_server_procs_per_node_cluster_d_cluster_c_cluster_e, num_server_procs_per_node_cluster_a_cluster_b,							  num_write_client_procs_per_node_cluster_d_cluster_c_cluster_e, num_write_client_procs_per_node_cluster_a_cluster_b,							  num_read_client_procs_per_node_cluster_d_cluster_c_cluster_e, num_read_client_procs_per_node_cluster_a_cluster_b );							    // add return value/tests to make sure things went okay?	// test_num_server_procs_per_node();}// ~~~!!!!!this is where I am leaving off  !!!!!~~~ - none of this has been modified for the new txn required format (or singularity execution commands)// (or any other changes)void generate_txn_file(bool repeat_jobs, string cluster, string run_type, uint32_t job_num, uint32_t iteration, uint32_t time, uint32_t num_timesteps, 						uint32_t total_nodes, uint32_t num_server_nodes, uint32_t num_write_client_nodes, uint32_t num_read_client_nodes,						uint32_t num_server_procs, uint32_t num_write_client_procs, uint32_t num_read_client_procs, 						uint32_t num_server_procs_per_node, uint32_t num_write_client_procs_per_node, uint32_t num_read_client_procs_per_node,						uint32_t num_write_x_procs, uint32_t num_write_y_procs, uint32_t num_write_z_procs,						uint32_t num_read_x_procs, uint32_t num_read_y_procs, uint32_t num_read_z_procs,						uint64_t nx, uint64_t ny, uint64_t nz						) {		extreme_debug_log << " server procs: " << num_server_procs << " num datasets: " << num_timesteps << " npx write: " << 				num_write_x_procs << " npy write: " << num_write_y_procs << " npz write: " << num_write_z_procs << endl;		uint32_t estm_num_time_pts_client = num_server_procs * (11) * num_timesteps * num_write_x_procs * num_write_y_procs * num_write_z_procs;		extreme_debug_log << "estm time pts client: " << estm_num_time_pts_client << endl;		extreme_debug_log << "about to create txn for " << cluster << " total num nodes: " << total_nodes << endl;		char write_server_cmd_line_args[256];		char read_server_cmd_line_args[256];		char write_client_cmd_line_args[256];		char read_client_cmd_line_args[256];		//dirman contact info, estm num time pts, bool load_db, bool output_db, job_id 		if (run_type == "reg") {			uint32_t estm_num_time_pts_server = .5 * estm_num_time_pts_client;			extreme_debug_log << "estm time pts server: " << estm_num_time_pts_server << endl;			snprintf(write_server_cmd_line_args, sizeof(write_server_cmd_line_args), "%s %d %d %d %s", 					"${MD_DIRMAN_CONTACT_INFO}", estm_num_time_pts_server, false, true, "$SLURM_JOBID");			snprintf(read_server_cmd_line_args, sizeof(read_server_cmd_line_args), "%s %d %d %d %s", 					"${MD_DIRMAN_CONTACT_INFO}", estm_num_time_pts_server, true, false, "$SLURM_JOBID");			snprintf(write_client_cmd_line_args, sizeof(write_client_cmd_line_args), "%s %d %d %d %d %d %d %d %d %d %s", 					"${MD_DIRMAN_CONTACT_INFO}", num_write_x_procs, num_write_y_procs, num_write_z_procs, 					nx, ny, nz, num_timesteps, estm_num_time_pts_client, num_server_procs, "$SLURM_JOBID");			snprintf(read_client_cmd_line_args, sizeof(read_client_cmd_line_args), "%s %d %d %d %d %d %d", 					"${MD_DIRMAN_CONTACT_INFO}", num_read_x_procs, num_read_y_procs, num_read_z_procs, 					num_timesteps, estm_num_time_pts_client, num_server_procs);		}		else {			snprintf(write_client_cmd_line_args, sizeof(write_client_cmd_line_args), "%d %d %d %d %d %d %d %d %s", 				num_write_x_procs, num_write_y_procs, num_write_z_procs, 				nx, ny, nz, num_timesteps, estm_num_time_pts_client, "$SLURM_JOBID");			if (run_type == "hdf5") {				snprintf(read_client_cmd_line_args, sizeof(read_client_cmd_line_args), "%d %d %d %d %d %d %d %d %s", 					num_read_x_procs, num_read_y_procs, num_read_z_procs, 					nx, ny, nz, num_timesteps, estm_num_time_pts_client, "$SLURM_JOBID");			}			else if (run_type == "local") {				snprintf(read_client_cmd_line_args, sizeof(read_client_cmd_line_args), "%d %d %d %d %d %s", 					num_read_x_procs, num_read_y_procs, num_read_z_procs, 					num_timesteps, estm_num_time_pts_client, "$SLURM_JOBID" );			}		}		bool cluster_b = !cluster.compare("cluster_b");		bool cluster_a = !cluster.compare("cluster_a");	 	bool cluster_d = !cluster.compare("cluster_d");	 	bool cluster_c = !cluster.compare("cluster_c");	 	bool cluster_e = !cluster.compare("cluster_e");		//todo - this will change for titan		string account = "FILL_IN_DESIRED_VALUE";		string write_testing_file_name, read_testing_file_name; 		string sbatch_path;		if (run_type == "reg") {		 	sbatch_path = "FILL_IN_DESIRED_VALUE";		 	write_testing_file_name = "testing_harness_new_write";		 	read_testing_file_name = "testing_harness_new_read";		}		else {			sbatch_path = "FILL_IN_DESIRED_VALUE";			write_testing_file_name = "testing_harness_write_" + run_type;		 	read_testing_file_name = "testing_harness_read_" + run_type;		}		string itr_to_print;		if(repeat_jobs) {			itr_to_print = to_string(iteration + 25);		}		else {			itr_to_print = to_string(iteration);		}		string testing_params;		if (run_type == "reg") {		 	testing_params = to_string(num_server_procs) + "_" + to_string(num_server_nodes) +	 						 "_" + to_string(num_write_client_procs) +  "_" + to_string(num_write_client_nodes) +	 						 "_" + to_string(num_read_client_procs) + "_" + to_string(num_read_client_nodes) +	 						 "_" + to_string(num_timesteps) + "_" + itr_to_print;		}		else {		 	testing_params = to_string(num_write_client_procs) +  "_" + to_string(num_write_client_nodes) +	 						 "_" + to_string(num_read_client_procs) + "_" + to_string(num_read_client_nodes) +	 						 "_" + to_string(num_timesteps) + "_" + itr_to_print;		}		string job_name = run_type + "_" + testing_params; 		string sbatch_file_name = testing_params + ".sl";		string sbatch_file_path = sbatch_path + "/" + sbatch_file_name;		debug_log << " sbatch_file_path: " << sbatch_file_path << endl;		string results_file_name = testing_params + ".log";		string sbatch_script_path= sbatch_path + "/"  + "sbatch_script";		debug_log << " results file path: " << sbatch_file_path << endl;		//todo - does titan use sbatch?		uint32_t hrs = time / 3600; 		uint32_t mins = (time - 3600*hrs) / 60;		uint32_t secs = time - 3600 * hrs - 60*mins; 		char time_str[30];		sprintf(time_str, "#SBATCH --time=%.2d:%.2d:%.2d", hrs, mins, secs);		ofstream file;		file.open(sbatch_file_path);		file << "#!/bin/bash" << endl << endl;		file << "#SBATCH --account=" << account << endl;		file << "#SBATCH --job-name=" << job_name << endl;		file << "#SBATCH --nodes=" << total_nodes << endl;		file << time_str << endl;		file << endl;		// file << "echo starting txn testing" << endl;		file << endl;		if (!local) {			if (run_type == "reg") {				file << "SOURCE_DIR=FILL_IN_DESIRED_VALUE";				file << "OUTPUT_DIR=FILL_IN_DESIRED_VALUE";				}			else {				file << "SOURCE_DIR=FILL_IN_DESIRED_VALUE" << endl;				file << "OUTPUT_DIR=FILL_IN_DESIRED_VALUE" << endl;								}		}		else { //being performed on local mac			if (run_type == "reg") {				file << "SOURCE_DIR=FILL_IN_DESIRED_VALUE" << endl;				file << "OUTPUT_DIR=FILL_IN_DESIRED_VALUE" << endl;			}			else {				file << "SOURCE_DIR=FILL_IN_DESIRED_VALUE" << endl;				file << "OUTPUT_DIR=FILL_IN_DESIRED_VALUE" << endl;					}			}		if (run_type == "reg") {			file << "NUM_SERVERS=" << num_server_procs << endl;			file << "NUM_SERVERS_PER_NODE=" << num_server_procs_per_node << endl;		}		file << "NUM_WRITE_CLIENTS=" << num_write_client_procs <<endl;		file << "NUM_WRITE_CLIENTS_PER_NODE=" << num_write_client_procs_per_node << endl;		file << "NUM_READ_CLIENTS=" << num_read_client_procs <<endl;		file << "NUM_READ_CLIENTS_PER_NODE=" << num_read_client_procs_per_node << endl;		file << endl;		// file << "cd $SOURCE_DIR" << endl;		// file << endl;		if (run_type == "reg") {			file << "DIRMAN_FILE_NAME=my_dirman" << endl;			file << "SERVER_FILE_NAME=my_metadata_server" << endl;		}		file << "WRITE_TESTING_FILE_NAME=" << write_testing_file_name << endl;		file << "READ_TESTING_FILE_NAME=" << read_testing_file_name << endl;  		file << endl;		if (run_type == "reg") {			file << "export MD_DIRMAN_CONTACT_INFO=${SOURCE_DIR}/md_dirman_contact_info_" << job_num << ".txt" << endl;			file << endl;			file << "WRITE_DIRMAN_LOG_FILE=${OUTPUT_DIR}/${DIRMAN_FILE_NAME}_write_" << 					results_file_name << endl;			file << "READ_DIRMAN_LOG_FILE=${OUTPUT_DIR}/${DIRMAN_FILE_NAME}_read_"<< 					results_file_name << endl;			file << "WRITE_SERVER_LOG_FILE=${OUTPUT_DIR}/${SERVER_FILE_NAME}_write_"<<  					results_file_name << endl;			file << "READ_SERVER_LOG_FILE=${OUTPUT_DIR}/${SERVER_FILE_NAME}_read_"<<					results_file_name << endl;		}		file << "WRITE_TESTING_LOG_FILE=${OUTPUT_DIR}/${WRITE_TESTING_FILE_NAME}_"<<				results_file_name << endl;		file << "READ_TESTING_LOG_FILE=${OUTPUT_DIR}/${READ_TESTING_FILE_NAME}_"<<  				results_file_name << endl;		file << endl;		if (run_type == "reg") {			file << "DIRMANNODE=`scontrol show hostname $SLURM_HOSTNAME | head -n 1`" << endl;			file << "SERVERNODES=`scontrol show hostname $SLURM_HOSTNAME | head -n " << 1+num_server_nodes << " | tail -n " << num_server_nodes << " | awk -vORS=, '{ print $1 }' | sed 's/,$//'`" << endl;		}		file << "WRITECLIENTNODES=`scontrol show hostname $SLURM_HOSTNAME | tail -n " << num_write_client_nodes << " | awk -vORS=, '{ print $1 }' | sed 's/,$//'`" << endl;		file << "READCLIENTNODES=`scontrol show hostname $SLURM_HOSTNAME | tail -n " << num_read_client_nodes << " | awk -vORS=, '{ print $1 }' | sed 's/,$//'`" << endl;		file << endl;		if(write_hdf5) {			file << "mkdir -p " << sbatch_path << "/XGC/$SLURM_JOBID" << endl;		}		file << endl;		file << "ALLNODES=`scontrol show hostname $SLURM_HOSTNAME`" << endl;		file << "echo \"$ALLNODES\" " << endl;		if(run_type == "reg") {			file << "echo \"mpiexec -nooversubscribe --host $DIRMANNODE -np 1 --npernode 1 ${SOURCE_DIR}/${DIRMAN_FILE_NAME} &> ${WRITE_DIRMAN_LOG_FILE} &\" " << endl;			file << "echo \"mpiexec -nooversubscribe --host $SERVERNODES -np ${NUM_SERVERS} --npernode ${NUM_SERVERS_PER_NODE} ${SOURCE_DIR}/${SERVER_FILE_NAME}\" " << endl;			file << "echo \"mpiexec -nooversubscribe --host $WRITECLIENTNODES -np ${NUM_WRITE_CLIENTS} --npernode ${NUM_WRITE_CLIENTS_PER_NODE} ${SOURCE_DIR}/${WRITE_TESTING_FILE_NAME}\" " << endl;			file << "echo \"mpiexec -nooversubscribe --host $DIRMANNODE -np 1 --npernode 1 ${SOURCE_DIR}/${DIRMAN_FILE_NAME} &> ${READ_DIRMAN_LOG_FILE} &\" " << endl;			file << "echo \"mpiexec -nooversubscribe --host $SERVERNODES -np ${NUM_SERVERS} --npernode ${NUM_SERVERS_PER_NODE} ${SOURCE_DIR}/${SERVER_FILE_NAME}\" " << endl;			file << "echo \"mpiexec -nooversubscribe --host $READCLIENTNODES -np ${NUM_READ_CLIENTS} --npernode ${NUM_READ_CLIENTS_PER_NODE} ${SOURCE_DIR}/${READ_TESTING_FILE_NAME}\" " << endl;		}		else {			file << "echo \"mpiexec -nooversubscribe --host $WRITECLIENTNODES -np ${NUM_WRITE_CLIENTS} --npernode ${NUM_WRITE_CLIENTS_PER_NODE} ${SOURCE_DIR}/${WRITE_TESTING_FILE_NAME}\" " << endl;			file << "echo \"mpiexec -nooversubscribe --host $READCLIENTNODES -np ${NUM_READ_CLIENTS} --npernode ${NUM_READ_CLIENTS_PER_NODE} ${SOURCE_DIR}/${READ_TESTING_FILE_NAME}\" " << endl;					}		if(run_type == "reg") {			file << "mpiexec -nooversubscribe --host $DIRMANNODE -np 1 --npernode 1 ${SOURCE_DIR}/${DIRMAN_FILE_NAME} &> ${WRITE_DIRMAN_LOG_FILE} & " << endl;			file << endl;			file << "mpiexec -nooversubscribe --host $SERVERNODES -np ${NUM_SERVERS} --npernode ${NUM_SERVERS_PER_NODE} ${SOURCE_DIR}/${SERVER_FILE_NAME} " <<					write_server_cmd_line_args << " &> ${WRITE_SERVER_LOG_FILE} &" << endl;			file << endl;		}		file << "mpiexec -nooversubscribe --host $WRITECLIENTNODES -np ${NUM_WRITE_CLIENTS} --npernode ${NUM_WRITE_CLIENTS_PER_NODE} ${SOURCE_DIR}/${WRITE_TESTING_FILE_NAME} " <<				write_client_cmd_line_args << " &> ${WRITE_TESTING_LOG_FILE} " << endl;		file << endl;		//after writing, the dirman and servers are shutdown, so relaunch		if(run_type == "reg") {			file << "sleep 60" << endl; //give the servers a chance to fully shutdown before proceeding			file << "export MD_DIRMAN_CONTACT_INFO=${SOURCE_DIR}/md_dirman_contact_info_" << job_num << "_new.txt" << endl;			file << "mpiexec -nooversubscribe --host $DIRMANNODE -np 1 --npernode 1 ${SOURCE_DIR}/${DIRMAN_FILE_NAME} &> ${READ_DIRMAN_LOG_FILE} & " << endl;			file << endl;			file << "mpiexec -nooversubscribe --host $SERVERNODES -np ${NUM_SERVERS} --npernode ${NUM_SERVERS_PER_NODE} ${SOURCE_DIR}/${SERVER_FILE_NAME} " <<					read_server_cmd_line_args << " &> ${READ_SERVER_LOG_FILE} &" << endl;			file << endl;		}		file << "mpiexec -nooversubscribe --host $READCLIENTNODES -np ${NUM_READ_CLIENTS} --npernode ${NUM_READ_CLIENTS_PER_NODE} ${SOURCE_DIR}/${READ_TESTING_FILE_NAME} " <<				read_client_cmd_line_args << " &> ${READ_TESTING_LOG_FILE} " << endl;		file << endl;		//makes sure we only keep one copy of the md dbs per configuration, hdf5 runs don't use an external db		if(iteration > 0 && run_type != "hdf5") { 			file << "rm -f " << sbatch_path << "/${SLURM_JOBID}_*" << endl;		}		//makes sure we don't store the (large) hdf5 files		if(write_hdf5) {			file << "rm -fr " << sbatch_path << "/XGC/$SLURM_JOBID/*" << endl;		}		file.close();		ofstream sbatch_script;		if(job_num == 0) {			sbatch_script.open(sbatch_script_path);				sbatch_script << "#! /bin/bash \n\n";			sbatch_script << "jid0=$(sbatch " << sbatch_file_name << " | awk '{print $4}')\n"; 		}		else {			sbatch_script.open(sbatch_script_path, std::ofstream::app);							sbatch_script << "jid"  << job_num << "=$(sbatch --dependency=afterany:$jid" << job_num-1 << " " << sbatch_file_name << " | awk '{print $4}')\n"; 		}		sbatch_script.close();	}void test_fixed_procs_per_node(uint32_t num_server_procs_per_node_cluster_d_cluster_c_cluster_e, uint32_t num_server_procs_per_node_cluster_a_cluster_b,							   uint32_t num_write_client_procs_per_node_cluster_d_cluster_c_cluster_e, uint32_t num_write_client_procs_per_node_cluster_a_cluster_b,							   uint32_t num_read_client_procs_per_node_cluster_d_cluster_c_cluster_e, uint32_t num_read_client_procs_per_node_cluster_a_cluster_b							   ) {	//remind me - was there a reason number of servers should be directly divisible by the number of clients? 	vector<uint32_t> standard_num_server_procs;	vector<uint32_t> extreme_num_server_procs;	// vector<uint32_t> num_write_client_procs = {1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576};	// vector<uint32_t> num_write_client_procs = {1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000, 256000, 512000, 1024000};	// vector<uint32_t> num_write_client_procs = {1000, 2000, 4000, 8000, 16000};	vector<uint32_t> num_write_client_procs = {4000, 8000};	// vector<uint32_t> num_write_client_procs = {2000};	// vector<uint32_t> num_write_client_procs = {100, 1000};	// vector<uint32_t> num_write_client_procs = {10000};	// vector<uint32_t> num_write_client_procs = {100};	// vector<string> run_types = {"hdf5"};	// vector<string> run_types = {"reg", "hdf5"};	// vector<string> run_types = {"reg"};	// vector<string> run_types = {"hdf5"};	vector<string> run_types = {"reg","hdf5"};	// vector<string> run_types = {"reg", "hdf5", "local"};	vector<uint32_t> num_read_client_procs;	for (uint32_t i = 0; i < num_write_client_procs.size(); i++) {		standard_num_server_procs.push_back( round( num_write_client_procs[i] / 100.0 ) );		extreme_num_server_procs.push_back( round( num_write_client_procs[i] / 1000.0 ) );		num_read_client_procs.push_back( round( num_write_client_procs[i] / 10.0 ) );	}	// uint32_t num_timesteps[] = {96, 288, 2880, 17280};	uint32_t num_timesteps[] = {3};	// uint32_t num_types[] = {0, 10};	// uint32_t num_types = 10;	uint32_t num_iterations = 6;	bool repeat_jobs = true;	bool skip_extreme = false;	bool cluster_b = true;	bool cluster_d = false;	bool cluster_c = false;	bool cluster_a = true;	bool cluster_e = false;	// uint32_t num_iterations = 1;	uint32_t num_vars = 10;	uint32_t bytes_per_data_pt = 8; //doubles are 8 bytes	uint32_t num_procs_opts = num_write_client_procs.size();	uint32_t num_timesteps_opts = sizeof(num_timesteps_opts) / sizeof(uint32_t);	// uint32_t num_types_opts = sizeof(num_types) / sizeof(uint32_t);	uint32_t num_write_x_procs[num_procs_opts];	uint32_t num_write_y_procs[num_procs_opts];	uint32_t num_write_z_procs[num_procs_opts];	uint32_t num_read_x_procs[num_procs_opts];	uint32_t num_read_y_procs[num_procs_opts];	uint32_t num_read_z_procs[num_procs_opts];	for(uint32_t i=0; i<num_procs_opts; i++) {		find_roots(num_write_client_procs[i], num_write_x_procs[i], num_write_y_procs[i], num_write_z_procs[i]);		find_roots(num_read_client_procs[i], num_read_x_procs[i], num_read_y_procs[i], num_read_z_procs[i]);	}	//note: cluster_c and cluster_d actually have 3.5, but better to have the same data per chunk	uint64_t RAM_per_proc = 4 * pow(10,9); //4 GB	uint64_t bytes_per_proc_per_timestep = .1 * RAM_per_proc; //10% of RAM	uint64_t bytes_per_chunk = bytes_per_proc_per_timestep / num_vars; 	uint64_t data_pts_per_chunk = bytes_per_chunk / bytes_per_data_pt; 	uint64_t ndx, ndy, ndz;	//note - can just do this once for all 4 clusters since they all have the same RAM/proc (4GB)	find_roots(data_pts_per_chunk, ndx, ndy, ndz);	cout << "data_pts_per_chunk: " << data_pts_per_chunk << " ndx: " << ndx << " ndy: " << ndy << " ndz: " << 			ndz << " confirming: ndx*ndy*ndz: " << ndx*ndy*ndz << endl;	// uint64_t cluster_a_cluster_b_RAM_per_node = 64 * pow(10,9);	// uint64_t cluster_a_cluster_b_RAM_per_proc = cluster_a_cluster_b_RAM_per_node / num_write_client_procs_per_node_cluster_a_cluster_b;	// uint64_t total_bytes_per_chunk = .1 * cluster_a_cluster_b_RAM_per_proc;	//todo - what are the rules for titan?	float max_perc_of_cluster = .5;	//note precise (they give varying numbers)	uint32_t max_num_nodes_cluster_a = FILL_IN_DESIRED_VALUE;	uint32_t max_num_nodes_cluster_d = FILL_IN_DESIRED_VALUE;	uint32_t max_num_nodes_cluster_c = FILL_IN_DESIRED_VALUE;	uint32_t max_num_nodes_cluster_b = FILL_IN_DESIRED_VALUE;	uint32_t max_num_nodes_cluster_e = FILL_IN_DESIRED_VALUE;	uint32_t max_num_processes_per_node_cluster_a_cluster_b = FILL_IN_DESIRED_VALUE;	uint32_t max_num_processes_per_node_cluster_d_cluster_c_cluster_e = FILL_IN_DESIRED_VALUE;	if (num_server_procs_per_node_cluster_d_cluster_c_cluster_e > max_num_processes_per_node_cluster_d_cluster_c_cluster_e ||		num_write_client_procs_per_node_cluster_d_cluster_c_cluster_e > max_num_processes_per_node_cluster_d_cluster_c_cluster_e ||		num_read_client_procs_per_node_cluster_d_cluster_c_cluster_e > max_num_processes_per_node_cluster_d_cluster_c_cluster_e ||		num_server_procs_per_node_cluster_a_cluster_b > max_num_processes_per_node_cluster_a_cluster_b ||		num_write_client_procs_per_node_cluster_a_cluster_b > max_num_processes_per_node_cluster_a_cluster_b ||		num_read_client_procs_per_node_cluster_a_cluster_b > max_num_processes_per_node_cluster_a_cluster_b ) {		cout << "error. requested " << 			num_server_procs_per_node_cluster_d_cluster_c_cluster_e << " server procs per node for cluster_d cluster_c " <<			num_write_client_procs_per_node_cluster_d_cluster_c_cluster_e << " write client procs per node for cluster_d cluster_c " <<			num_read_client_procs_per_node_cluster_d_cluster_c_cluster_e << " read client procs per node for cluster_d cluster_c " <<			num_server_procs_per_node_cluster_a_cluster_b << " server procs per node for cluster_a cluster_b " <<			num_write_client_procs_per_node_cluster_a_cluster_b << " write client procs per node for cluster_a cluster_b " << 			num_read_client_procs_per_node_cluster_a_cluster_b << " read client procs per node for cluster_a cluster_b " << endl;		return;	}	string cluster;	uint32_t time;	for (string run_type : run_types) {		uint32_t job_num_cluster_a=0;		uint32_t job_num_cluster_d=0;		uint32_t job_num_cluster_c=0;		uint32_t job_num_cluster_b=0;		uint32_t job_num_cluster_e = 0;		for (uint32_t j=0; j<num_procs_opts; j++) {			for (uint32_t use_standard_num_server_procs = 0; use_standard_num_server_procs<= 1; use_standard_num_server_procs++) {				for (uint32_t k=0; k<num_timesteps_opts; k++) {					// for (uint32_t l=0; l<num_types_opts; l++) { //no need to test 0 types any more					uint32_t my_num_server_procs;					if (use_standard_num_server_procs) {						my_num_server_procs = standard_num_server_procs[j];						if(run_type == "hdf5" || run_type == "local") {							continue; //hdf5 and local don't have a varying number of servers so only need to run once per config						}					}					else {						my_num_server_procs = extreme_num_server_procs[j];						if(skip_extreme) {							continue;						}					}					if (my_num_server_procs == 0) {						continue;					}					uint32_t x_length_per_proc = ndx; 					uint32_t y_length_per_proc = ndy;					uint32_t z_length_per_proc = ndz; 					uint32_t nx = x_length_per_proc * num_write_x_procs[j];					uint32_t ny = y_length_per_proc * num_write_y_procs[j];					uint32_t nz = z_length_per_proc * num_write_z_procs[j];					uint32_t my_num_write_client_procs = num_write_client_procs[j];					uint32_t my_num_read_client_procs = num_read_client_procs[j];					if ( (nx % num_read_x_procs[j] != 0) || 						 (ny % num_read_y_procs[j] != 0) || 						 (nz % num_read_z_procs[j] != 0) ) 					{						debug_log << "global dimensions not divisible by number of read procs. finding nearest divisible \n";						find_nearest_divisible ( nx, ny, nz, my_num_read_client_procs, 							num_read_x_procs[j], num_read_y_procs[j], num_read_z_procs[j]);					}						if (num_write_x_procs[j] * num_write_y_procs[j] * num_write_z_procs[j] != my_num_write_client_procs ||						num_read_x_procs[j] * num_read_y_procs[j] * num_read_z_procs[j] != my_num_read_client_procs ) {						cout << "error. requested write_x procs: " << num_write_x_procs[j] << " write_y procs: " << num_write_y_procs[j] <<							" write_z procs: " << num_write_z_procs[j] << " and read x procs: " << num_read_x_procs[j] <<							" read y procs: " << num_read_y_procs[j] << " read z procs: " << num_read_z_procs[j] << endl;						cout << "this muliplies to " << (num_write_x_procs[j] * num_write_y_procs[j] * num_write_z_procs[j] ) <<							" write procs instead of the requested " << my_num_write_client_procs << " and " <<							(num_read_x_procs[j] * num_read_y_procs[j] * num_read_z_procs[j] ) << " instead of the requested " <<							my_num_read_client_procs << endl;					}					extreme_debug_log << "j: " << j << " use_standard_num_server_procs: " << use_standard_num_server_procs << 						" k: " << endl;					if (use_standard_num_server_procs == 1 && k == 0 ) {						cout << "for " << my_num_write_client_procs << " write clients, the standard number of servers is " << 							standard_num_server_procs[j] << ", the extreme number of servers is " << extreme_num_server_procs[j] << 							", the number of read clients is " << num_read_client_procs[j] << endl;						cout <<	" npx for write: " << num_write_x_procs[j] << " npy for write: " << num_write_y_procs[j] <<							" and npz for write: " << num_write_z_procs[j] << endl;						cout <<	" npx for read: " << num_read_x_procs[j] << " npy for read: " << num_read_y_procs[j] <<							" and npz for read: " << num_read_z_procs[j] << endl;					}					uint32_t num_server_nodes_cluster_d_cluster_c_cluster_e, num_server_nodes_cluster_a_cluster_b, num_dirman_nodes;					if (run_type == "reg") {						num_server_nodes_cluster_d_cluster_c_cluster_e = ceil( my_num_server_procs / (float)num_server_procs_per_node_cluster_d_cluster_c_cluster_e );						num_server_nodes_cluster_a_cluster_b =  ceil ( my_num_server_procs / (float)num_server_procs_per_node_cluster_a_cluster_b );						num_dirman_nodes = 1;					}					else {						num_server_nodes_cluster_d_cluster_c_cluster_e = 0;						num_server_nodes_cluster_a_cluster_b = 0;						num_dirman_nodes = 0;					}					uint32_t num_write_client_nodes_cluster_d_cluster_c_cluster_e = ceil ( my_num_write_client_procs / (float)num_write_client_procs_per_node_cluster_d_cluster_c_cluster_e );					uint32_t num_write_client_nodes_cluster_a_cluster_b = ceil ( my_num_write_client_procs / (float)num_write_client_procs_per_node_cluster_a_cluster_b );					uint32_t num_read_client_nodes_cluster_d_cluster_c_cluster_e = ceil ( my_num_read_client_procs / (float)num_read_client_procs_per_node_cluster_d_cluster_c_cluster_e );					uint32_t num_read_client_nodes_cluster_a_cluster_b = ceil ( my_num_read_client_procs / (float)num_read_client_procs_per_node_cluster_a_cluster_b );					extreme_debug_log << "num_write_client_nodes_cluster_a_cluster_b: " << num_write_client_nodes_cluster_a_cluster_b << " num_read_client_nodes_cluster_a_cluster_b: " <<						num_read_client_nodes_cluster_a_cluster_b << endl;					uint32_t total_nodes_cluster_d_cluster_c_cluster_e = num_server_nodes_cluster_d_cluster_c_cluster_e + 						max(num_write_client_nodes_cluster_d_cluster_c_cluster_e, num_read_client_nodes_cluster_d_cluster_c_cluster_e) + num_dirman_nodes; 					uint32_t total_nodes_cluster_a_cluster_b = num_server_nodes_cluster_a_cluster_b + 						max(num_write_client_nodes_cluster_a_cluster_b, num_read_client_nodes_cluster_a_cluster_b) + num_dirman_nodes;					uint32_t my_num_timesteps = num_timesteps[k];					extreme_debug_log << "total_nodes_cluster_a_cluster_b: " << total_nodes_cluster_a_cluster_b << endl;					//fix - how much time does it need these days?					time = 5400; 					if (my_num_write_client_procs >= 2000) {						time *= 2;						if (my_num_write_client_procs >= 4000) {							time *= 3;							if (my_num_write_client_procs >= 8000) {								time *= 4;							}						}					}					if (run_type == "hdf5") { //the hdf5 runs seem to be taking 30%+ longer						time *= 2;					}					if (time > 86400) {						time = 86400; //24 hour limit! 					}					if (cluster_a && total_nodes_cluster_a_cluster_b <= max_num_nodes_cluster_a) {											cluster = "cluster_a";						for(uint32_t iteration=0; iteration<num_iterations; iteration++) {							generate_txn_file(repeat_jobs, cluster, run_type, job_num_cluster_a, iteration, time, my_num_timesteps, 							total_nodes_cluster_a_cluster_b, num_server_nodes_cluster_a_cluster_b, num_write_client_nodes_cluster_a_cluster_b, 							num_read_client_nodes_cluster_a_cluster_b, my_num_server_procs, my_num_write_client_procs, my_num_read_client_procs,							min ( num_server_procs_per_node_cluster_a_cluster_b, my_num_server_procs), num_write_client_procs_per_node_cluster_a_cluster_b, 							min( num_read_client_procs_per_node_cluster_a_cluster_b, my_num_read_client_procs),							num_write_x_procs[j], num_write_y_procs[j], num_write_z_procs[j],							num_read_x_procs[j], num_read_y_procs[j], num_read_z_procs[j],							nx, ny, nz							);							job_num_cluster_a++;						}					}					if (cluster_d && total_nodes_cluster_d_cluster_c_cluster_e <= max_num_nodes_cluster_d) {													cluster = "cluster_d";						for(uint32_t iteration=0; iteration<num_iterations; iteration++) {							generate_txn_file(repeat_jobs, cluster, run_type, job_num_cluster_d, iteration, time, my_num_timesteps, 							total_nodes_cluster_d_cluster_c_cluster_e, num_server_nodes_cluster_d_cluster_c_cluster_e, num_write_client_nodes_cluster_d_cluster_c_cluster_e, 							num_read_client_nodes_cluster_d_cluster_c_cluster_e, my_num_server_procs, my_num_write_client_procs, my_num_read_client_procs, 							min( num_server_procs_per_node_cluster_d_cluster_c_cluster_e, my_num_server_procs), num_write_client_procs_per_node_cluster_d_cluster_c_cluster_e, 							min( num_read_client_procs_per_node_cluster_d_cluster_c_cluster_e, my_num_read_client_procs),							num_write_x_procs[j], num_write_y_procs[j], num_write_z_procs[j],							num_read_x_procs[j], num_read_y_procs[j], num_read_z_procs[j],							nx, ny, nz							);							job_num_cluster_d++;						}					}					if (cluster_c && total_nodes_cluster_d_cluster_c_cluster_e <= max_num_nodes_cluster_c) {													cluster = "cluster_c";						for(uint32_t iteration=0; iteration<num_iterations; iteration++) {							generate_txn_file(repeat_jobs, cluster, run_type, job_num_cluster_c, iteration, time, my_num_timesteps,							total_nodes_cluster_d_cluster_c_cluster_e, num_server_nodes_cluster_d_cluster_c_cluster_e, num_write_client_nodes_cluster_d_cluster_c_cluster_e, 							num_read_client_nodes_cluster_d_cluster_c_cluster_e, my_num_server_procs, my_num_write_client_procs, my_num_read_client_procs,							min (num_server_procs_per_node_cluster_d_cluster_c_cluster_e, my_num_server_procs), num_write_client_procs_per_node_cluster_d_cluster_c_cluster_e, 							min( num_read_client_procs_per_node_cluster_d_cluster_c_cluster_e, my_num_read_client_procs),							num_write_x_procs[j], num_write_y_procs[j], num_write_z_procs[j],							num_read_x_procs[j], num_read_y_procs[j], num_read_z_procs[j],							nx, ny, nz							);							job_num_cluster_c++;						}					}					if (cluster_b && total_nodes_cluster_a_cluster_b <= max_num_nodes_cluster_b) {													cluster = "cluster_b";						for(uint32_t iteration=0; iteration<num_iterations; iteration++) {							generate_txn_file(repeat_jobs, cluster, run_type, job_num_cluster_b, iteration, time, my_num_timesteps,							total_nodes_cluster_a_cluster_b, num_server_nodes_cluster_a_cluster_b, num_write_client_nodes_cluster_a_cluster_b, 							num_read_client_nodes_cluster_a_cluster_b, my_num_server_procs, my_num_write_client_procs, my_num_read_client_procs,							min( num_server_procs_per_node_cluster_a_cluster_b, my_num_server_procs), num_write_client_procs_per_node_cluster_a_cluster_b, 							min( num_read_client_procs_per_node_cluster_a_cluster_b, my_num_read_client_procs),							num_write_x_procs[j], num_write_y_procs[j], num_write_z_procs[j],							num_read_x_procs[j], num_read_y_procs[j], num_read_z_procs[j],							nx, ny, nz														);							job_num_cluster_b++;						}					}					if (cluster_e && total_nodes_cluster_d_cluster_c_cluster_e <= max_num_nodes_cluster_e) {													cluster = "cluster_e";						for(uint32_t iteration=0; iteration<num_iterations; iteration++) {							generate_txn_file(repeat_jobs, cluster, run_type, job_num_cluster_e, iteration, time, my_num_timesteps,							total_nodes_cluster_d_cluster_c_cluster_e, num_server_nodes_cluster_d_cluster_c_cluster_e, num_write_client_nodes_cluster_d_cluster_c_cluster_e, 							num_read_client_nodes_cluster_d_cluster_c_cluster_e, my_num_server_procs, my_num_write_client_procs, my_num_read_client_procs,							min (num_server_procs_per_node_cluster_d_cluster_c_cluster_e, my_num_server_procs), num_write_client_procs_per_node_cluster_d_cluster_c_cluster_e, 							min( num_read_client_procs_per_node_cluster_d_cluster_c_cluster_e, my_num_read_client_procs),							num_write_x_procs[j], num_write_y_procs[j], num_write_z_procs[j],							num_read_x_procs[j], num_read_y_procs[j], num_read_z_procs[j],							nx, ny, nz							);							job_num_cluster_e++;						}					}				}			}		}	}}// void find_roots(uint32_t value, uint32_t &npx, uint32_t &npy, uint32_t &npz) {template <class T>void find_roots(T value, T &npx, T &npy, T &npz) {	T incr = cbrt(value);	T decr = cbrt(value);	while(true) {		debug_log << "value: " << value << " incr: " << incr << " decr: " << decr << endl;		if (value % decr == 0) {			npx = decr;			break;		}		else if(value % incr == 0) {			npx = incr;			break;		}		incr += 1;		decr -= 1;	}	value = value / npx;	incr = sqrt( value );	decr = sqrt( value );	while( true ) {		debug_log << "value: " << value << " npx: " << npx << " incr: " << incr << " decr: " << decr << endl;		if(value % incr == 0) {			npy = incr;			break;		}		else if (value % decr == 0) {			npy = decr;			break;		}		incr += 1;		decr -= 1;	}		npz = value / npy;	debug_log << "value: " << value << " npx: " << npx << " npy: " << npy << endl;	if(npy < npx) {		T temp = npy;		npy = npx;		npx = temp;	}	if(npz < npy) {		T temp = npy;		npy = npz;		npz = temp;	}	debug_log << "value: " << value*npx << " npx: " << npx << " npy: " << npy << " npz: " << npz << endl;}// void find_next_roots(uint32_t value, uint32_t &npx, uint32_t &npy, uint32_t &npz) {template <class T>void find_next_roots(T value, T &npx, T &npy, T &npz) {	T incr = npx+1;	T decr = npx-1;	while(true) {		extreme_debug_log << "value: " << value << " incr: " << incr << " decr: " << decr << endl;		if (value % decr == 0) {			npx = decr;			break;		}		else if(value % incr == 0) {			npx = incr;			break;		}		incr += 1;		decr -= 1;	}	value = value / npx;	incr = sqrt( value );	decr = sqrt( value );	while( true ) {		extreme_debug_log << "value: " << value << " npx: " << npx << " incr: " << incr << " decr: " << decr << endl;		if(value % incr == 0) {			npy = incr;			break;		}		else if (value % decr == 0) {			npy = decr;			break;		}		incr += 1;		decr -= 1;	}	npz = value / npy;	if(npy < npx) {		T temp = npy;		npy = npx;		npx = temp;	}	if(npz < npy) {		T temp = npy;		npy = npz;		npz = temp;	}	extreme_debug_log << "value: " << value*npx << " npx: " << npx << " npy: " << npy << " npz: " << npz << endl;}// void find_nearest_divisible (uint32_t nx, uint32_t ny, uint32_t nz, uint32_t &num_procs, uint32_t &npx, uint32_t &npy, uint32_t &npz) {template <class T>void find_nearest_divisible (T nx, T ny, T nz, T &num_procs, T &npx, T &npy, T &npz) {	T incr = num_procs + 1;	T decr = num_procs - 1;	extreme_debug_log << "num read_procs: " << num_procs << " npx: " << npx << 		" npy: " << npy << " npz: " << npz << endl;	extreme_debug_log << "nx: " << nx << " ny: " << ny << " nz: " << nz << endl;		find_next_roots(num_procs, npx, npy, npz);	debug_log << "less balanced roots: npx: " << npx << " npy: " << npy << " npz: " << npz << endl;	if ( (nx % npx == 0) && (ny % npy == 0) && (nz % npz != 0) ) {		debug_log << "for the less balanced roots, npx,npy,npz do not match. continuing " << endl;	}	else if (npx * 3 < npz ) {		debug_log << "for the less balanced roots, npx,npy,npz match but npx and npz are more than 3x different. continuing " << endl;	}	else { 		debug_log << "for the less balanced roots, npx,npy,npz match and npx and npz are sufficiently close. returning " << endl;		return;	}	while ( true ) {		T incr_cbrt = cbrt(incr);		find_roots(incr, npx, npy, npz);		extreme_debug_log << "incr: " << incr << " incr npx: " << npx << " incr npy: " << npy <<			" incr npz: " << npz << endl;		if( (nx % npx == 0) && (ny % npy == 0) && (nz % npz == 0) ) {			num_procs = incr;			extreme_debug_log << "incr matches!" << endl;			break;		}		T decr_cbrt = cbrt(decr);		while(decr % decr_cbrt != 0) {			decr_cbrt -= 1;		}		T decr_rt2 = sqrt( decr / decr_cbrt );		while( (decr / decr_cbrt) % decr_rt2 != 0) {			decr_rt2 += 1;		}		npx = decr_cbrt;		npy = decr_rt2;		npz = decr / (decr_cbrt * decr_rt2);		if(npz < npy) {			T temp = npy;			npy = npz;			npz = temp;		}		extreme_debug_log << "decr: " << decr << " decr npx: " << npx << " decr npy: " << npy <<			" decr npz: " << npz << endl;		if( (nx % npx != 0) || (ny % npy != 0) || (nz % npz != 0) ) {			incr += 1;			decr -= 1;		}		else {			extreme_debug_log << "decr matches!" << endl;			num_procs = decr;			break;		}	}}