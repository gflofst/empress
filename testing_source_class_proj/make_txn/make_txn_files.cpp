#include <iostream>#include <fstream>#include <string>#include <stdlib.h>#include <math.h>       /* cbrt */#include <algorithm>    // std::min#include "testing_configs.hh"#include <map>using namespace std;// bool local = false;bool debug_logging = false;// bool debug_logging = false;// bool extreme_debug_logging = false;bool extreme_debug_logging = false;// bool write_hdf5 = true;// static debugLog debug_log = debugLog(debug_logging);// static debugLog extreme_debug_log = debugLog(extreme_debug_logging);debugLog debug_log = debugLog(debug_logging);debugLog extreme_debug_log = debugLog(extreme_debug_logging);// uint32_t num_iterations;uint32_t num_exec_copies = 10;extern void find_config(uint32_t num_write_procs, uint32_t num_read_procs, uint32_t num_data_pts_per_proc,                 uint64_t &my_nx, uint64_t &my_ny, uint64_t &my_nz,                uint32_t &my_npx, uint32_t &my_npy, uint32_t &my_npz,                 uint32_t &my_npx_read, uint32_t &my_npy_read, uint32_t &my_npz_read                );void get_time(md_write_type write_type, md_server_type server_type, md_db_index_type index_type, md_db_checkpoint_type checkpt_type, bool do_read,    uint32_t my_num_write_client_procs,  uint32_t &time);void test_fixed_procs_per_node(uint32_t num_server_procs_per_node_cluster_c_d_e, uint32_t num_server_procs_per_node_cluster_a_b,                               uint32_t num_write_client_procs_per_node_cluster_c_d_e, uint32_t num_write_client_procs_per_node_cluster_a_b,                               uint32_t num_read_client_procs_per_node_cluster_c_d_e, uint32_t num_read_client_procs_per_node_cluster_a_b                               );void generate_txn_file(bool repeat_jobs, string cluster, uint32_t job_num, uint32_t iteration, uint32_t time, uint32_t num_timesteps,                        uint32_t total_nodes, uint32_t num_server_nodes, uint32_t num_write_client_nodes, uint32_t num_read_client_nodes,                        uint32_t num_server_procs, uint32_t num_write_client_procs, uint32_t num_read_client_procs,                        uint32_t num_server_procs_per_node, uint32_t num_write_client_procs_per_node, uint32_t num_read_client_procs_per_node,                        uint32_t num_write_x_procs, uint32_t num_write_y_procs, uint32_t num_write_z_procs,                        uint32_t num_read_x_procs, uint32_t num_read_y_procs, uint32_t num_read_z_procs,                        uint64_t nx, uint64_t ny, uint64_t nz, uint32_t num_timesteps_per_checkpt,                        md_write_type write_type, md_server_type server_type, md_db_index_type index_type, md_db_checkpoint_type checkpt_type, bool do_read,                        int num_completed_iterations, bool sep_executable, bool multiple_executables                        );// void get_sim_space_and_num_read_procs(uint64_t ndx, uint64_t ndy, uint64_t ndz, int j, const uint32_t *num_write_x_procs,//     const uint32_t *num_write_y_procs, const uint32_t *num_write_z_procs,//     uint32_t *num_read_x_procs, uint32_t *num_read_y_procs, uint32_t *num_read_z_procs,//     uint32_t &my_num_write_client_procs, uint32_t &my_num_read_client_procs,//     uint64_t &nx, uint64_t &ny, uint64_t &nz//     );// template <class T1, class T2>// void find_nearest_divisible (T1 nx, T1 ny, T1 nz, T2 &num_read_procs, T2 &num_read_x_procs, T2 &num_read_y_procs, T2 &num_read_z_procs);// template <class T>// void find_roots(T value, T &npx, T &npy, T &npz);int main(int argc, char **argv) {    uint32_t num_server_procs_per_node_cluster_c_d_e = FILL_IN_WITH_DESIRED_VALUE;    uint32_t num_server_procs_per_node_cluster_a_b = FILL_IN_WITH_DESIRED_VALUE;    uint32_t num_write_client_procs_per_node_cluster_c_d_e = FILL_IN_WITH_DESIRED_VALUE;    uint32_t num_read_client_procs_per_node_cluster_c_d_e = FILL_IN_WITH_DESIRED_VALUE;    uint32_t num_write_client_procs_per_node_cluster_a_b = FILL_IN_WITH_DESIRED_VALUE;    uint32_t num_read_client_procs_per_node_cluster_a_b = FILL_IN_WITH_DESIRED_VALUE;    test_fixed_procs_per_node(num_server_procs_per_node_cluster_c_d_e, num_server_procs_per_node_cluster_a_b,                              num_write_client_procs_per_node_cluster_c_d_e, num_write_client_procs_per_node_cluster_a_b,                              num_read_client_procs_per_node_cluster_c_d_e, num_read_client_procs_per_node_cluster_a_b );                                // add return value/tests to make sure things went okay?    // test_num_server_procs_per_node();}// ~~~!!!!!this is where I am leaving off  !!!!!~~~ - none of this has been modified for the new txn required format (or singularity execution commands)// (or any other changes)bool new_transaction_test(md_server_type server_type, md_db_index_type index_type) {    return (        server_type == SERVER_DEDICATED_SQLITE_TRANSACTION_MANAGEMENT_WAL ||        server_type == SERVER_DEDICATED_SQLITE_TRANSACTION_MANAGEMENT_DB_STREAMS ||        server_type == SERVER_DEDICATED_D2T_TRANSACTION_MANAGEMENT ||        index_type == INDEX_RTREE    );}void generate_txn_file(bool repeat_jobs, string cluster, uint32_t job_num, uint32_t iteration, uint32_t time, uint32_t num_timesteps,                        uint32_t total_nodes, uint32_t num_server_nodes, uint32_t num_write_client_nodes, uint32_t num_read_client_nodes,                        uint32_t num_server_procs, uint32_t num_write_client_procs, uint32_t num_read_client_procs,                        uint32_t num_server_procs_per_node, uint32_t num_write_client_procs_per_node, uint32_t num_read_client_procs_per_node,                        uint32_t num_write_x_procs, uint32_t num_write_y_procs, uint32_t num_write_z_procs,                        uint32_t num_read_x_procs, uint32_t num_read_y_procs, uint32_t num_read_z_procs,                        uint64_t nx, uint64_t ny, uint64_t nz, uint32_t num_timesteps_per_checkpt,                        md_write_type write_type, md_server_type server_type, md_db_index_type index_type, md_db_checkpoint_type checkpt_type, bool do_read,                        int num_completed_iterations, bool sep_executable, bool multiple_executables                        ){        extreme_debug_log << " server procs: " << num_server_procs << " num datasets: " << num_timesteps << " npx write: " <<                 num_write_x_procs << " npy write: " << num_write_y_procs << " npz write: " << num_write_z_procs << endl;        uint32_t estm_num_time_pts_client = num_server_procs * (11) * num_timesteps * num_write_x_procs * num_write_y_procs * num_write_z_procs;        extreme_debug_log << "estm time pts client: " << estm_num_time_pts_client << endl;        extreme_debug_log << "about to create txn for " << cluster << " total num nodes: " << total_nodes << endl;        char server_cmd_line_args[256];        char client_cmd_line_args[256];        bool local = false;        uint32_t exec_count = job_num % num_exec_copies + num_completed_iterations;        //dirman contact info, estm num time pts, bool load_db, bool output_db, job_id         if(write_type != WRITE_HDF5) {            uint32_t estm_num_time_pts_server = .5 * estm_num_time_pts_client;            extreme_debug_log << "estm time pts server: " << estm_num_time_pts_server << endl;            snprintf(server_cmd_line_args, sizeof(server_cmd_line_args), "%s %d %d %s %d %d %d",                     "${MD_DIRMAN_CONTACT_INFO}", estm_num_time_pts_server, false, "$SLURM_JOBID", server_type, index_type, checkpt_type);            if( server_type != SERVER_LOCAL_IN_MEM && server_type != SERVER_LOCAL_ON_DISK) {                snprintf(client_cmd_line_args, sizeof(client_cmd_line_args), "%s %d %d %d %d %d %d %d %d %d %s %d %d %d %d %d %d %d %d",                        "${MD_DIRMAN_CONTACT_INFO}", num_write_x_procs, num_write_y_procs, num_write_z_procs,                        nx, ny, nz, num_timesteps, estm_num_time_pts_client, num_server_procs, "$SLURM_JOBID",                        num_read_x_procs, num_read_y_procs, num_read_z_procs, num_timesteps_per_checkpt,                        write_type, server_type, checkpt_type, do_read);            }            else {                snprintf(client_cmd_line_args, sizeof(client_cmd_line_args), "%d %d %d %d %d %d %d %d %s %d %d %d %d %d",                        num_write_x_procs, num_write_y_procs, num_write_z_procs,                        nx, ny, nz, num_timesteps, estm_num_time_pts_client, "$SLURM_JOBID",                        num_timesteps_per_checkpt, server_type, index_type, checkpt_type, do_read);                    local = true;                        }        }        else {            snprintf(client_cmd_line_args, sizeof(client_cmd_line_args), "%d %d %d %d %d %d %d %d %s %d %d %d %d",                 num_write_x_procs, num_write_y_procs, num_write_z_procs,                 nx, ny, nz, num_timesteps, estm_num_time_pts_client, "$SLURM_JOBID",                 num_read_x_procs, num_read_y_procs, num_read_z_procs, num_timesteps_per_checkpt);            // else if (run_type == "local") {            //     snprintf(read_client_cmd_line_args, sizeof(read_client_cmd_line_args), "%d %d %d %d %d %s",             //         num_read_x_procs, num_read_y_procs, num_read_z_procs,             //         num_timesteps, estm_num_time_pts_client, "$SLURM_JOBID" );            // }        }        bool is_new_transaction_test = new_transaction_test(server_type, index_type);               string sbatch_path;        sbatch_path = "FILL_IN_WITH_DESIRED_VALUE";        if(local) {            testing_file_name = "testing_harness_class_proj_local";        }        else if(server_type == SERVER_DEDICATED_SQLITE_TRANSACTION_MANAGEMENT_WAL) {            testing_file_name = "testing_harness_class_proj_sqlite_transactions_only";        }        else if(server_type == SERVER_DEDICATED_SQLITE_TRANSACTION_MANAGEMENT_DB_STREAMS) {            testing_file_name = "testing_harness_class_proj_sqlite_transactions_only_db_streams";        }        else if(server_type == SERVER_DEDICATED_D2T_TRANSACTION_MANAGEMENT) {            testing_file_name = "testing_harness_class_proj_d2t_transactions_only";        }        else if (write_type == WRITE_REG || write_type == WRITE_GATHERED) {             testing_file_name = "testing_harness_class_proj";        }        else if (write_type == WRITE_LARGE_MD_VOL) {             testing_file_name = "testing_harness_large_md_vol";        }        else if(write_type == WRITE_HDF5) {             testing_file_name = "testing_harness_class_proj_hdf5";             testing_file_name_to_exec = "testing_harness_class_proj_hdf5";            }        else {            cout << "error. the testing type didn't fall into one of the expected categories" << endl;            return;        }        if(server_type == SERVER_DEDICATED_SQLITE_TRANSACTION_MANAGEMENT_WAL) {            server_file_name = "my_metadata_server_sqlite_transactions_only";        }        else if(server_type == SERVER_DEDICATED_SQLITE_TRANSACTION_MANAGEMENT_DB_STREAMS) {            server_file_name = "my_metadata_server_sqlite_transactions_only_db_streams";        }        else if(server_type == SERVER_DEDICATED_D2T_TRANSACTION_MANAGEMENT) {            server_file_name = "my_metadata_server_d2t_transactions_only";        }        else if(index_type == INDEX_RTREE) {            server_file_name = "my_metadata_server_rtree";        }        else {            server_file_name = "my_metadata_server";                    }        string itr_to_print;        if(repeat_jobs) {            itr_to_print = to_string(iteration + num_completed_iterations);        }        else {            itr_to_print = to_string(iteration);        }        string testing_params;        // if (run_type == "reg") {             testing_params = to_string(num_server_procs) + "_" + to_string(num_server_nodes) +                              "_" + to_string(num_write_client_procs) +  "_" + to_string(num_write_client_nodes) +                              "_" + to_string(num_read_client_procs) + "_" + to_string(num_read_client_nodes) +                              "_" + to_string(num_timesteps) + "_" + to_string(num_timesteps_per_checkpt) +                              "_" + to_string(write_type) + "_" + to_string(server_type) + "_" + to_string(index_type) + "_" + to_string(checkpt_type) +                               "_" + itr_to_print;        // }        // else {        //      testing_params = to_string(num_write_client_procs) +  "_" + to_string(num_write_client_nodes) +     //                          "_" + to_string(num_read_client_procs) + "_" + to_string(num_read_client_nodes) +     //                          "_" + to_string(num_timesteps) + "_" + itr_to_print;        // }        // string job_name = run_type + "_" + testing_params;         // string job_name = "(" + to_string(write_type) + "," + to_string(server_type) + "," + to_string(index_type) + "," + to_string(checkpt_type) + "," + itr_to_print + ")";         string job_name = to_string(write_type) + "," + to_string(server_type) + "," + to_string(index_type) + "," + to_string(checkpt_type) + "," + itr_to_print;         string sbatch_file_name = testing_params + ".sl";        string sbatch_file_path = sbatch_path + "/" + sbatch_file_name;        debug_log << " sbatch_file_path: " << sbatch_file_path << endl;        string results_file_name = testing_params + ".log";        string sbatch_script_path = sbatch_path + "/";        if(is_new_transaction_test) {            sbatch_script_path += "sbatch_script_transaction_tests";        }        else {            sbatch_script_path += "sbatch_script";                    }        debug_log << " results file path: " << sbatch_file_path << endl;        //todo - does titan use sbatch?        uint32_t hrs = time / 3600;         uint32_t mins = (time - 3600*hrs) / 60;        uint32_t secs = time - 3600 * hrs - 60*mins;         char time_str[30];        sprintf(time_str, "#SBATCH --time=%.2d:%.2d:%.2d", hrs, mins, secs);        ofstream file;        file.open(sbatch_file_path);        file << "#!/bin/bash" << endl << endl;        file << "#SBATCH --account=" << "FILL_IN_WITH_DESIRED_VALUE" << endl;                file << "#SBATCH --job-name=" << job_name << endl;            file << "#SBATCH --nodes=" << total_nodes << endl;        file << time_str << endl;        file << endl;        // file << "echo starting txn testing" << endl;        file << endl;        // if (!local) {            // if (run_type == "reg") {        if(is_new_transaction_test) {            file << "SOURCE_DIR=/" << "FILL_IN_WITH_DESIRED_VALUE" << endl;        }        else {            file << "SOURCE_DIR=/" << "FILL_IN_WITH_DESIRED_VALUE" << endl;                    }        file << "OUTPUT_DIR=/" << "FILL_IN_WITH_DESIRED_VALUE" << endl;            if (write_type != WRITE_HDF5 && !local) {            file << "NUM_SERVERS=" << num_server_procs << endl;            file << "NUM_SERVERS_PER_NODE=" << num_server_procs_per_node << endl;        }        file << "NUM_WRITE_CLIENTS=" << num_write_client_procs <<endl;        file << "NUM_WRITE_CLIENTS_PER_NODE=" << num_write_client_procs_per_node << endl;        file << "NUM_READ_CLIENTS=" << num_read_client_procs <<endl;        file << "NUM_READ_CLIENTS_PER_NODE=" << num_read_client_procs_per_node << endl;        file << endl;        // file << "cd $SOURCE_DIR" << endl;        // file << endl;        if(multiple_executables) {            file << "TESTING_FILE_NAME=" << testing_file_name + to_string(exec_count) << endl;            if (write_type != WRITE_HDF5 && !local) {                file << "DIRMAN_FILE_NAME=my_dirman" << exec_count << endl;                // file << "SERVER_FILE_NAME=my_metadata_server" << exec_count << endl;                file << "SERVER_FILE_NAME=" << server_file_name << exec_count << endl;            }                    }        else {            if (write_type != WRITE_HDF5 && !local) {                file << "DIRMAN_FILE_NAME=my_dirman" << endl;                // file << "SERVER_FILE_NAME=my_metadata_server" << endl;                file << "SERVER_FILE_NAME=" << server_file_name << endl;            }            file << "TESTING_FILE_NAME=" << testing_file_name << endl;                                }        if(sep_executable) {            file << "TESTING_FILE_NAME_TO_EXEC=" << testing_file_name_to_exec << endl;        }        // file << "READ_TESTING_FILE_NAME=" << read_testing_file_name << endl;          file << endl;        if (write_type != WRITE_HDF5 && !local) {            file << "export MD_DIRMAN_CONTACT_INFO=${SOURCE_DIR}/md_dirman_contact_info_" << job_num << ".txt" << endl;            file << endl;            file << "DIRMAN_LOG_FILE=${OUTPUT_DIR}/my_dirman_" <<                     results_file_name << endl;            // file << "READ_DIRMAN_LOG_FILE=${OUTPUT_DIR}/${DIRMAN_FILE_NAME}_read_"<<             //         results_file_name << endl;            // file << "SERVER_LOG_FILE=${OUTPUT_DIR}/my_metadata_server_"<<            file << "SERVER_LOG_FILE=${OUTPUT_DIR}/" << server_file_name << "_"<<                        results_file_name << endl;            // file << "READ_SERVER_LOG_FILE=${OUTPUT_DIR}/${SERVER_FILE_NAME}_read_"<<            //         results_file_name << endl;        }        file << "TESTING_LOG_FILE=${OUTPUT_DIR}/" << testing_file_name << "_"<<                results_file_name << endl;            if(do_read && num_write_client_procs == 1000 && iteration == 0) {                cout << "read: " << testing_file_name << "_" << results_file_name << endl;            }        // file << "READ_TESTING_LOG_FILE=${OUTPUT_DIR}/${READ_TESTING_FILE_NAME}_"<<          //         results_file_name << endl;        file << endl;            if (write_type != WRITE_HDF5 && !local) {                file << "DIRMANNODE=`scontrol show hostname $SLURM_HOSTNAME | head -n 1`" << endl;                file << "SERVERNODES=`scontrol show hostname $SLURM_HOSTNAME | head -n " << 1+num_server_nodes << " | tail -n " << num_server_nodes << " | awk -vORS=, '{ print $1 }' | sed 's/,$//'`" << endl;            }            file << "CLIENTNODES=`scontrol show hostname $SLURM_HOSTNAME | tail -n " << num_write_client_nodes << " | awk -vORS=, '{ print $1 }' | sed 's/,$//'`" << endl;            // file << "READCLIENTNODES=`scontrol show hostname $SLURM_HOSTNAME | tail -n " << num_read_client_nodes << " | awk -vORS=, '{ print $1 }' | sed 's/,$//'`" << endl;        // }            file << endl;        if(write_type == WRITE_HDF5) {            file << "mkdir -p " << sbatch_path << "/XGC/$SLURM_JOBID" << endl;        }        file << endl;        file << "ALLNODES=`scontrol show hostname $SLURM_HOSTNAME`" << endl;        file << "echo \"$ALLNODES\" " << endl;        if(write_type != WRITE_HDF5 && !local) {            file << "echo \"mpiexec -nooversubscribe --host $DIRMANNODE -np 1 --npernode 1 ${SOURCE_DIR}/${DIRMAN_FILE_NAME} &> ${DIRMAN_LOG_FILE} &\" " << endl;            file << "echo \"mpiexec -nooversubscribe --host $SERVERNODES -np ${NUM_SERVERS} --npernode ${NUM_SERVERS_PER_NODE} ${SOURCE_DIR}/${SERVER_FILE_NAME} " <<                     server_cmd_line_args << " &> ${SERVER_LOG_FILE} &\" " << endl;            // file << "echo \"mpiexec -nooversubscribe --host $DIRMANNODE -np 1 --npernode 1 ${SOURCE_DIR}/${DIRMAN_FILE_NAME} &> ${READ_DIRMAN_LOG_FILE} &\" " << endl;            // file << "echo \"mpiexec -nooversubscribe --host $SERVERNODES -np ${NUM_SERVERS} --npernode ${NUM_SERVERS_PER_NODE} ${SOURCE_DIR}/${SERVER_FILE_NAME}\" " << endl;            // file << "echo \"mpiexec -nooversubscribe --host $READCLIENTNODES -np ${NUM_READ_CLIENTS} --npernode ${NUM_READ_CLIENTS_PER_NODE} ${SOURCE_DIR}/${READ_TESTING_FILE_NAME}\" " << endl;        }        file << "echo \"mpiexec -nooversubscribe --host $CLIENTNODES -np ${NUM_WRITE_CLIENTS} --npernode ${NUM_WRITE_CLIENTS_PER_NODE} ${SOURCE_DIR}/${TESTING_FILE_NAME} " <<                   client_cmd_line_args << " &> ${TESTING_LOG_FILE} \" " << endl;        // else {        //     file << "echo \"mpiexec -nooversubscribe --host $CLIENTNODES -np ${NUM_WRITE_CLIENTS} --npernode ${NUM_WRITE_CLIENTS_PER_NODE} ${SOURCE_DIR}/${TESTING_FILE_NAME}\" " << endl;        //     file << "echo \"mpiexec -nooversubscribe --host $READCLIENTNODES -np ${NUM_READ_CLIENTS} --npernode ${NUM_READ_CLIENTS_PER_NODE} ${SOURCE_DIR}/${READ_TESTING_FILE_NAME}\" " << endl;                    // }        file << endl;        if(write_type != WRITE_HDF5 && !local) {            file << "mpiexec -nooversubscribe --host $DIRMANNODE -np 1 --npernode 1 ${SOURCE_DIR}/${DIRMAN_FILE_NAME} &> ${DIRMAN_LOG_FILE} & " << endl;            file << "sleep 3" << endl;            file << endl;            file << "mpiexec -nooversubscribe --host $SERVERNODES -np ${NUM_SERVERS} --npernode ${NUM_SERVERS_PER_NODE} ${SOURCE_DIR}/${SERVER_FILE_NAME} " <<                    server_cmd_line_args << " &> ${SERVER_LOG_FILE} &" << endl;            file << endl;        }        file << "mpiexec -nooversubscribe --host $CLIENTNODES -np ${NUM_WRITE_CLIENTS} --npernode ${NUM_WRITE_CLIENTS_PER_NODE} ";        if(sep_executable) {            file << "${SOURCE_DIR}/${TESTING_FILE_NAME_TO_EXEC} ";        }        else {            file << "${SOURCE_DIR}/${TESTING_FILE_NAME} ";        }        file << client_cmd_line_args << " &> ${TESTING_LOG_FILE} " << endl;        file << endl;        //after writing, the dirman and servers are shutdown, so relaunch        //makes sure we only keep one copy of the md dbs per configuration, hdf5 runs don't use an external db        if(write_type != WRITE_HDF5 && !local) {            file << "wait" << endl;        }        if(!(iteration == 0 && checkpt_type == 0 && write_type != WRITE_HDF5)) {             // file << "rm -f " << sbatch_path << "/${SLURM_JOBID}_*" << endl;            // file << "find " << sbatch_path << " -name '" << "${SLURM_JOBID}_*' -print0 | xargs -0 -P0 rm -f" << endl;            file << "find " << sbatch_path << " -name '${SLURM_JOBID}_*' -print0 | xargs -0 -P0 rm -f" << endl;        }        if(write_type != WRITE_HDF5 && !local) {            file << "rm -f ${SOURCE_DIR}/md_dirman_contact_info_" << job_num << ".txt" << endl;        }        //don't store the hdf5 files        else {            if(iteration != 0) {                file << "rm -fr " << sbatch_path << "/XGC/$SLURM_JOBID" << endl;            }        }        // file << "rm -f *.out" << endl; //will want these in the event of program failure        file.close();        ofstream sbatch_script;        if(job_num == 0) {            sbatch_script.open(sbatch_script_path);                sbatch_script << "#! /bin/bash \n\n";            sbatch_script << "jid0=$(sbatch " << sbatch_file_name << " | awk '{print $4}')\n";         }        else if(multiple_executables) {            sbatch_script.open(sbatch_script_path, std::ofstream::app);                // cout << "multiple_executables, num_exec_copies: " << num_exec_copies << ", job_num: " << job_num << endl;            if(job_num < num_exec_copies) {                // cout << "true, " << job_num << " < " << num_exec_copies << endl;                 sbatch_script << "jid"  << job_num << "=$(sbatch " << sbatch_file_name << " | awk '{print $4}')\n";             }            else {                // sbatch_script << "jid"  << job_num << "=$(sbatch --dependency=afterok:$jid" << job_num-num_exec_copies << " " << sbatch_file_name << " | awk '{print $4}')\n";                 sbatch_script << "jid"  << job_num << "=$(sbatch --dependency=afterany:$jid" << job_num-num_exec_copies << " " << sbatch_file_name << " | awk '{print $4}')\n";             }        }        else {            sbatch_script.open(sbatch_script_path, std::ofstream::app);              // sbatch_script << "jid"  << job_num << "=$(sbatch --dependency=afterok:$jid" << job_num-1 << " " << sbatch_file_name << " | awk '{print $4}')\n";                           sbatch_script << "jid"  << job_num << "=$(sbatch --dependency=afterany:$jid" << job_num-1 << " " << sbatch_file_name << " | awk '{print $4}')\n";         }        sbatch_script.close();            exec_count += 1;        exec_count = exec_count % num_exec_copies;}void test_fixed_procs_per_node(uint32_t num_server_procs_per_node_cluster_c_d_e, uint32_t num_server_procs_per_node_cluster_a_b,                               uint32_t num_write_client_procs_per_node_cluster_c_d_e, uint32_t num_write_client_procs_per_node_cluster_a_b,                               uint32_t num_read_client_procs_per_node_cluster_c_d_e, uint32_t num_read_client_procs_per_node_cluster_a_b                               ) {    // vector<uint32_t> num_write_client_procs = {1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576};    // vector<uint32_t> num_write_client_procs = {1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000, 256000, 512000, 1024000};    // vector<uint32_t> num_write_client_procs = {1000, 2000, 4000, 8000, 16000};    // vector<uint32_t> num_write_client_procs = {4000, 8000};    // vector<uint32_t> num_write_client_procs = {2000};/*----------------------------------------------------------------------------------------------------------------------------parameters to set------------------------------------------------------------------------------------------------------------*/    // vector<uint32_t> num_write_client_procs = {200};    bool make_specific = true;    bool do_new_tests = true;    bool do_old_tests = true;    vector<testing_config> configs;    bool repeat_jobs = false;    int num_completed_iterations = 0;    bool tiny_debug_run = false;    // bool debug_run = true;    bool debug_run = false;    bool single_run = false;    vector<uint32_t> num_write_client_procs;    bool skip_extreme = false, skip_standard = false;    bool cluster_a = false, cluster_b = false, cluster_c = false, cluster_d = false, cluster_e = false;    bool sep_executable = false;    bool multiple_executables = false;    uint32_t num_timesteps = 1000;    uint32_t num_timesteps_per_checkpt = 100;    uint32_t num_timesteps_large_md_vol = 100;    uint32_t num_timesteps_per_checkpt_large_md_vol = 10;    vector<uint32_t> standard_num_server_procs;    vector<uint32_t> extreme_num_server_procs;    vector<uint32_t> num_read_client_procs;    // uint32_t num_iterations = 6;    // uint32_t num_iterations = 5;    uint32_t num_iterations = 5;    if(tiny_debug_run) {        num_write_client_procs = {20};        skip_extreme = true;        num_iterations = 1;        num_timesteps = 20;        num_timesteps_per_checkpt = 10;        cluster_e = true;        multiple_executables = true;    }    else if(debug_run) {        num_write_client_procs = {100};        skip_extreme = true; //since we only have 100 client procs        cluster_c = true;        num_iterations = 1;        num_timesteps = 1000;        num_timesteps_per_checkpt = 100;    }    else if(single_run) {        skip_extreme = true;    }    else { //testing at large scale        num_write_client_procs = {1000, 2000, 4000, 8000, 16000};        skip_standard = true;        cluster_d = true;        cluster_e = true;    }    if(do_new_tests) {        cout << "am doing new tests" << endl;        if(make_specific) {            configs = get_specific_new_testing_configs();        }        else if(tiny_debug_run || debug_run) {            configs = get_new_testing_configs(num_write_client_procs[0], num_iterations);        }        else {            configs = get_new_testing_configs();                        }    }    if(do_old_tests) {        vector<testing_config> configs_old;        if(make_specific) {            cout << "am making specific slurm scripts" << endl;            configs_old = get_specific_testing_configs();        }        else {            cout << "am making all slurm scripts" << endl;              configs_old = get_testing_configs();              }        if(do_new_tests) {            configs.insert(configs.end(), configs_old.begin(), configs_old.end() );        }        else {            configs = configs_old;        }    }/*-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------*/    uint32_t num_vars = 10;    uint32_t large_md_num_vars = 100;    uint32_t bytes_per_data_pt = 8; //doubles are 8 bytes    uint32_t num_procs_opts = num_write_client_procs.size();    uint32_t num_write_x_procs, num_write_y_procs, num_write_z_procs;    uint32_t num_read_x_procs, num_read_y_procs, num_read_z_procs;    uint32_t reg_num_write_x_procs, reg_num_write_y_procs, reg_num_write_z_procs;    uint32_t reg_num_read_x_procs, reg_num_read_y_procs, reg_num_read_z_procs;    uint32_t large_md_num_write_x_procs, large_md_num_write_y_procs, large_md_num_write_z_procs;    uint32_t large_md_num_read_x_procs, large_md_num_read_y_procs, large_md_num_read_z_procs;    uint64_t RAM_per_proc = 4 * pow(10,9); //4 GB    uint64_t bytes_per_proc_per_timestep = .1 * RAM_per_proc; //10% of RAM    uint64_t bytes_per_chunk = bytes_per_proc_per_timestep / num_vars;     uint64_t data_pts_per_chunk = bytes_per_chunk / bytes_per_data_pt;     uint64_t large_md_bytes_per_chunk = bytes_per_proc_per_timestep / large_md_num_vars;     uint64_t large_md_data_pts_per_chunk = large_md_bytes_per_chunk / bytes_per_data_pt;         uint64_t ndx, ndy, ndz;    uint64_t large_md_ndx, large_md_ndy, large_md_ndz;    float max_perc_of_cluster = .5;    int FILL_IN_WITH_DESIRED_VALUE = 1000;    //note - not precise (they give varying numbers online)    uint32_t max_num_nodes_cluster_a = FILL_IN_WITH_DESIRED_VALUE;    uint32_t max_num_nodes_cluster_b = FILL_IN_WITH_DESIRED_VALUE;    uint32_t max_num_nodes_cluster_c = FILL_IN_WITH_DESIRED_VALUE;    uint32_t max_num_nodes_cluster_d = FILL_IN_WITH_DESIRED_VALUE;    uint32_t max_num_nodes_cluster_e = FILL_IN_WITH_DESIRED_VALUE;    string cluster;    uint32_t time;    extreme_debug_log << "about to start generating txn files" << endl;    // for (string run_type : run_types) {    uint32_t job_num_cluster_a=0;    uint32_t job_num_cluster_b=0;    uint32_t job_num_cluster_c=0;    uint32_t job_num_cluster_d=0;    uint32_t job_num_cluster_e = 0;    for (uint32_t i=0; i<num_procs_opts; i++) {        uint32_t my_num_write_client_procs = num_write_client_procs[i];        uint32_t my_num_read_client_procs = my_num_write_client_procs/10;        uint64_t nx, ny, nz;        uint64_t large_md_nx, large_md_ny, large_md_nz;        uint64_t reg_nx, reg_ny, reg_nz;        cout << "for " << my_num_write_client_procs << " write clients, ";        if(!skip_standard) {            cout << "the standard number of servers is " << my_num_write_client_procs/100 << ", ";        }        if(!skip_extreme) {            cout << "the extreme number of servers is " << my_num_write_client_procs/1000 << ", ";        }        cout << "the number of read clients is " << my_num_read_client_procs << endl;        find_config(my_num_write_client_procs, my_num_read_client_procs, data_pts_per_chunk,            reg_nx, reg_ny, reg_nz,             reg_num_write_x_procs, reg_num_write_y_procs, reg_num_write_z_procs,             reg_num_read_x_procs, reg_num_read_y_procs, reg_num_read_z_procs            );        cout << "reg md: " << endl;        cout << "npx: " << reg_num_write_x_procs << " npy: " << reg_num_write_y_procs << " npz: " << reg_num_write_z_procs << endl;        cout << "npx_read: " << reg_num_read_x_procs << " npy_read: " << reg_num_read_y_procs << " npz_read: " << reg_num_read_z_procs << endl;        cout << "nx: " << reg_nx << " ny: " << reg_ny << " nz: " << reg_nz << endl;        cout << endl;        find_config(my_num_write_client_procs, my_num_read_client_procs, large_md_data_pts_per_chunk,            large_md_nx, large_md_ny, large_md_nz,             large_md_num_write_x_procs, large_md_num_write_y_procs, large_md_num_write_z_procs,             large_md_num_read_x_procs, large_md_num_read_y_procs, large_md_num_read_z_procs            );        cout << "large md: " << endl;        cout << "npx: " << large_md_num_write_x_procs << " npy: " << large_md_num_write_y_procs << " npz: " << large_md_num_write_z_procs << endl;        cout << "npx_read: " << large_md_num_read_x_procs << " npy_read: " << large_md_num_read_y_procs << " npz_read: " << large_md_num_read_z_procs << endl;        cout << "nx: " << large_md_nx << " ny: " << large_md_ny << " nz: " << large_md_nz << endl;        cout << endl;        for(testing_config config : configs) {            for (uint32_t j=0; j<config.num_procs.size(); j++) {                if(config.num_procs[j] > my_num_write_client_procs) {                    break;                }                else if(config.num_procs[j] != my_num_write_client_procs) {                    continue;                }                    if(tiny_debug_run) {                    num_write_x_procs = 2; num_write_y_procs = 2; num_write_z_procs = 5;                    num_read_x_procs = 1; num_read_y_procs = 1; num_read_z_procs = 2;                    ndx = 10; ndy = 10; ndz = 10;                //  num_write_x_procs = 2; num_write_y_procs = 2; num_write_z_procs = 5;                //  num_read_x_procs = 1; num_read_y_procs = 1; num_read_z_procs = 2;                    nx = ndx * num_write_x_procs; ny = ndy * num_write_y_procs; nz = ndz * num_write_z_procs;                }                else {                    if(config.write_type == WRITE_LARGE_MD_VOL) {                        nx = large_md_nx;                        ny = large_md_ny;                        nz = large_md_nz;                        num_write_x_procs = large_md_num_write_x_procs;                        num_write_y_procs = large_md_num_write_y_procs;                        num_write_z_procs = large_md_num_write_z_procs;                        num_read_x_procs = large_md_num_read_x_procs;                        num_read_y_procs = large_md_num_read_y_procs;                        num_read_z_procs = large_md_num_read_z_procs;                    }                    else {                        nx = reg_nx;                        ny = reg_ny;                        nz = reg_nz;                        num_write_x_procs = reg_num_write_x_procs;                        num_write_y_procs = reg_num_write_y_procs;                        num_write_z_procs = reg_num_write_z_procs;                        num_read_x_procs = reg_num_read_x_procs;                        num_read_y_procs = reg_num_read_y_procs;                        num_read_z_procs = reg_num_read_z_procs;                    }                }                for (uint32_t use_standard_num_server_procs = 0; use_standard_num_server_procs<= 1; use_standard_num_server_procs++) {                    // for (uint32_t k=0; k<num_timesteps_opts; k++) {                    uint32_t my_num_server_procs;                    if (use_standard_num_server_procs) {                        if( (config.write_type == WRITE_HDF5 && !skip_extreme) || skip_standard) {                            continue; //hdf5 and local don't have a varying number of servers so only need to run once per config                        }                        extreme_debug_log << "am using standard num server procs" << endl;                        my_num_server_procs = my_num_write_client_procs/100;                            }                    else {                        if(skip_extreme) {                            continue;                        }                        my_num_server_procs = my_num_write_client_procs/1000;                    }                    if (my_num_server_procs == 0) {                        continue;                    }                    uint32_t num_server_nodes_cluster_c_d_e, num_server_nodes_cluster_a_b, num_dirman_nodes;                    if (config.write_type != WRITE_HDF5) {                        num_server_nodes_cluster_c_d_e = ceil( my_num_server_procs / (float)num_server_procs_per_node_cluster_c_d_e );                        num_server_nodes_cluster_a_b =  ceil ( my_num_server_procs / (float)num_server_procs_per_node_cluster_a_b );                        num_dirman_nodes = 1;                    }                    else {                        num_server_nodes_cluster_c_d_e = 0;                        num_server_nodes_cluster_a_b = 0;                        num_dirman_nodes = 0;                    }                    uint32_t my_num_timesteps = num_timesteps;                    uint32_t my_num_timesteps_per_checkpt = num_timesteps_per_checkpt;                    if(config.write_type == WRITE_LARGE_MD_VOL) {                        my_num_timesteps = num_timesteps_large_md_vol;                        my_num_timesteps_per_checkpt = num_timesteps_per_checkpt_large_md_vol;                    }                    uint32_t num_write_client_nodes_cluster_c_d_e = ceil ( my_num_write_client_procs / (float)num_write_client_procs_per_node_cluster_c_d_e );                    uint32_t num_write_client_nodes_a_b = ceil ( my_num_write_client_procs / (float)num_write_client_procs_per_node_cluster_a_b );                    uint32_t num_read_client_nodes_cluster_c_d_e = ceil ( my_num_read_client_procs / (float)num_read_client_procs_per_node_cluster_c_d_e );                    uint32_t num_read_client_nodes_a_b = ceil ( my_num_read_client_procs / (float)num_read_client_procs_per_node_cluster_a_b );                    extreme_debug_log << "num_write_client_nodes_a_b: " << num_write_client_nodes_a_b << " num_read_client_nodes_a_b: " <<                        num_read_client_nodes_a_b << endl;                    uint32_t total_nodes_cluster_c_d_e = num_server_nodes_cluster_c_d_e +                         max(num_write_client_nodes_cluster_c_d_e, num_read_client_nodes_cluster_c_d_e) + num_dirman_nodes;                     uint32_t total_nodes_cluster_a_b,= num_server_nodes_cluster_a_b +                         max(num_write_client_nodes_a_b, num_read_client_nodes_cluster_a_b) + num_dirman_nodes;                    extreme_debug_log << "total_nodes_cl : " << total_nodes_cl  << endl;                    //fix - how much time does it need these days?                    // time = 5400;                     //want the output files to print the same for hdf5 even though it doesn't actually use a server                    if(config.write_type == WRITE_HDF5) {                        num_server_nodes_cluster_c_d_e = ceil( my_num_server_procs / (float)num_server_procs_per_node_cluster_c_d_e );                        num_server_nodes_cluster_a_b =  ceil ( my_num_server_procs / (float)num_server_procs_per_node_cluster_a_b );                    }                                        get_time(config.write_type, config.server_type, config.index_type, config.checkpt_type, config.do_read, my_num_write_client_procs, time);                    // uint32_t num_timesteps = num_timesteps;                    cout << "write_type: " << config.write_type << " checkpt_type: " << config.checkpt_type << " server_type: "                         << config.server_type << " index_type: " << config.index_type << endl;                    if (cluster_a && total_nouster_a_b,s_cl  <= max_num_nodes_cluster_a) {                                          cluster = "cluster_a";                        for(uint32_t iteration=0; iteration<config.iterations[j]; iteration++) {                            generate_txn_file(repeat_jobs, cluster, job_num_cluster_a, iteration, time, my_num_timesteps,                            total_nodes_cluster_a_b, num_server_nodes_cluster_a_b, num_write_client_nodes_a_b,                            num_read_client_nodes_a_b, my_num_server_procs, my_num_write_client_procs, my_num_read_client_procs,                            min ( num_server_procs_per_node_cluster_a_b, my_num_server_procs), num_write_client_procs_per_node_cluster_a_b,                            min( num_read_client_procs_per_node_cluster_a_b, my_num_read_client_procs),                            num_write_x_procs, num_write_y_procs, num_write_z_procs,                            num_read_x_procs, num_read_y_procs, num_read_z_procs,                            nx, ny, nz, my_num_timesteps_per_checkpt,                            config.write_type, config.server_type, config.index_type, config.checkpt_type, config.do_read,                            num_completed_iterations, sep_executable, multiple_executables                            );                            job_num_cluster_a++;                        }                    }                    if (cluster_d && total_nodes_cluster_c_d_e <= max_num_nodes_cluster_b) {                                                    cluster = "cluster_d";                        for(uint32_t iteration=0; iteration<config.iterations[j]; iteration++) {                            generate_txn_file(repeat_jobs, cluster, job_num_cluster_b, iteration, time, my_num_timesteps,                            total_nodes_cluster_c_d_e, num_server_nodes_cluster_c_d_e, num_write_client_nodes_cluster_c_d_e,                            num_read_client_nodes_cluster_c_d_e, my_num_server_procs, my_num_write_client_procs, my_num_read_client_procs,                            min( num_server_procs_per_node_cluster_c_d_e, my_num_server_procs), num_write_client_procs_per_node_cluster_c_d_e,                            min( num_read_client_procs_per_node_cluster_c_d_e, my_num_read_client_procs),                            num_write_x_procs, num_write_y_procs, num_write_z_procs,                            num_read_x_procs, num_read_y_procs, num_read_z_procs,                            nx, ny, nz, my_num_timesteps_per_checkpt,                            config.write_type, config.server_type, config.index_type, config.checkpt_type, config.do_read,                            num_completed_iterations, sep_executable, multiple_executables                            );                            job_num_cluster_b++;                        }                    }                    if (cluster_c && total_nodes_cluster_c_d_e <= max_num_nodes_cluster_c) {                                                    cluster = "cluster_c";                        for(uint32_t iteration=0; iteration<config.iterations[j]; iteration++) {                            generate_txn_file(repeat_jobs, cluster, job_num_cluster_c, iteration, time, my_num_timesteps,                            total_nodes_cluster_c_d_e, num_server_nodes_cluster_c_d_e, num_write_client_nodes_cluster_c_d_e,                            num_read_client_nodes_cluster_c_d_e, my_num_server_procs, my_num_write_client_procs, my_num_read_client_procs,                            min (num_server_procs_per_node_cluster_c_d_e, my_num_server_procs), num_write_client_procs_per_node_cluster_c_d_e,                            min( num_read_client_procs_per_node_cluster_c_d_e, my_num_read_client_procs),                            num_write_x_procs, num_write_y_procs, num_write_z_procs,                            num_read_x_procs, num_read_y_procs, num_read_z_procs,                            nx, ny, nz, my_num_timesteps_per_checkpt,                            config.write_type, config.server_type, config.index_type, config.checkpt_type, config.do_read,                            num_completed_iterations, sep_executable, multiple_executables                            );                            job_num_cluster_c++;                        }                    }                    if (cluster_b && total_nouster_a_b,s_cl  <= max_num_nodes_cluster_d) {                                                  cluster = "cluster_b";                        for(uint32_t iteration=0; iteration<config.iterations[j]; iteration++) {                            generate_txn_file(repeat_jobs, cluster, job_num_cluster_b, iteration, time, my_num_tidddps,                            total_nodes_cluster_a_b, num_server_nodes_cluster_a_b, num_write_client_nodes_a_b,                            num_read_client_nodes_a_b, my_num_server_procs, my_num_write_client_procs, my_num_read_client_procs,                            min( num_server_procs_per_node_cluster_a_b, my_num_server_procs), num_write_client_procs_per_node_cluster_a_b,                            min( num_read_client_procs_per_node_cluster_a_b, my_num_read_client_procs),                            num_write_x_procs, num_write_y_procs, num_write_z_procs,                            num_read_x_procs, num_read_y_procs, num_read_z_procs,                            nx, ny, nz, my_num_timesteps_per_checkpt,                            config.write_type, config.server_type, config.index_type, config.checkpt_type, config.do_read,                            num_completed_iterations, sep_executable, multiple_executables                                                      );                            job_num_cluster_d++;                        }                    }                    if (cluster_e && total_nodes_cluster_c_d_e <= max_num_nodes_cluster_e) {                                                    cluster = "cluster_e";                        for(uint32_t iteration=0; iteration<config.iterations[j]; iteration++) {                            generate_txn_file(repeat_jobs, cluster, job_num_cluster_e, iteration, time, my_num_timesteps,                            total_nodes_cluster_c_d_e, num_server_nodes_cluster_c_d_e, num_write_client_nodes_cluster_c_d_e,                            num_read_client_nodes_cluster_c_d_e, my_num_server_procs, my_num_write_client_procs, my_num_read_client_procs,                            min (num_server_procs_per_node_cluster_c_d_e, my_num_server_procs), num_write_client_procs_per_node_cluster_c_d_e,                            min( num_read_client_procs_per_node_cluster_c_d_e, my_num_read_client_procs),                            num_write_x_procs, num_write_y_procs, num_write_z_procs,                            num_read_x_procs, num_read_y_procs, num_read_z_procs,                            nx, ny, nz, my_num_timesteps_per_checkpt,                            config.write_type, config.server_type, config.index_type, config.checkpt_type, config.do_read,                            num_completed_iterations, sep_executable, multiple_executables                                                  );                            job_num_cluster_e++;                        }                    }                }            }        }    }}void get_time(md_write_type write_type, md_server_type server_type, md_db_index_type index_type, md_db_checkpoint_type checkpt_type, bool do_read,    uint32_t my_num_write_client_procs,  uint32_t &time){    // time = 5400;    time = 2000;    // time = 1000;    // time = my_num_write_client_procs;    // time = (5.4)*my_num_write_client_procs;    // time = (4.5)*my_num_write_client_procs;    // time = (7.2)*my_num_write_client_procs;    // time = (10.8)*my_num_write_client_procs;    if (my_num_write_client_procs >= 2000) {        time *= 1.5;        // if (my_num_write_client_procs >= 4000) {        //     time *= 1.5;        //     if (my_num_write_client_procs >= 8000) {        //         time *= 1.5;        //     }        // }    }    if(server_type == SERVER_DEDICATED_ON_DISK ||  server_type == SERVER_LOCAL_ON_DISK) {        time *= 16;    }    //tends to take a bit longer    else if(server_type == SERVER_DEDICATED_SQLITE_TRANSACTION_MANAGEMENT_DB_STREAMS) {        time *= 2;    }    if(write_type == WRITE_HDF5) {        time *= 48;    }    else if(write_type == WRITE_LARGE_MD_VOL) {        time *= 10;    }    else if(write_type == WRITE_GATHERED) {        time *= 2;    }    if(do_read) { //will take extra time if we have to do reads        time *= 2;        if(index_type == WRITE_DELAYED_INDEX || index_type == INDEX_RTREE) { //will take extra time if we dont have indices for reads            time *= 2;        }            }    if (time > 86400) {        time = 86400; //24 hour limit!     }    }